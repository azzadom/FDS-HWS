{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1171fb23",
   "metadata": {
    "id": "qSfK3TzzOeBK",
    "tags": []
   },
   "source": [
    "# **#2 Homework: Classification**\n",
    "\n",
    "**Fundamentals of Data Science - Winter Semester 2024**\n",
    "\n",
    "##### Matteo Migliarini (TA), Matteo Rampolla (TA) and Prof. Indro Spinelli\n",
    "<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it>, <spinelli@di.uniroma1.it>\n",
    "\n",
    "---\n",
    "\n",
    "*Note: your task is to fill in the missing code where you see `\"YOUR CODE HERE\"` and the text part `\"WRITE YOUR TEXT HERE\"` part corresponding to each subproblem and produce brief reports on the results whenever necessary. Note also that a part of this missing code is also distributed in the python files in the folder `libs/`*\n",
    "\n",
    "As part of the homework, provide the answer to questions in this notebook report-like manner. \n",
    "\n",
    "After you have implemented all the missing code in the required sections, you will be able to run all the code without any errors. \n",
    "\n",
    "We kindly ask you to double-check this since **all** the delivered homework will be executed.\n",
    "\n",
    "The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "**Submit it** by sending an email to:\n",
    "\n",
    "<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it> and <spinelli@di.uniroma1.it> **by 29th November, 23:59**.\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "**Outline and Scores for #2 Homework:**\n",
    "\n",
    "\n",
    "* **Question 1: Logistic Regression** *(6 points)*\n",
    "  * **1.1: Log-likelihood and Gradient Ascent rule** (1 points)\n",
    "  * **1.2: Implementation of Logistic Regression with Gradient Ascent** (2 points)\n",
    "  * **1.3: Report** (3 points)\n",
    "* **Question 2: Polynomial Expansion** *(7 points)*\n",
    "  * **2.1: Polynomial features for logistic regression** (1 points)\n",
    "  * **2.2: Plot the computed non-linear boundary** (2 point)\n",
    "  * **2.4: Penalization** (4 points)\n",
    "* **Question 3: Multinomial Classification** *(9  points)*\n",
    "  * **3.1: Softmax Regression Model** (1 point)\n",
    "  * **3.2: Coding** (3 points)\n",
    "  * **3.3: Pipeline** (2 point)\n",
    "  * **3.4: Hyperparameters** (1 point)\n",
    "  * **3.5: Report** (2 point)\n",
    "* **Question 4: First approach to CNNs** *(8 points)*\n",
    "  * **4.1: Split the CIFAR-10 dataset** (1 point)\n",
    "  * **4.2: Identify and Correct Errors in the CNN Model** (3 points)\n",
    "  * **4.3: Training procedure** (2 points)\n",
    "  * **4.4: Evaluate** (1 point)\n",
    "  * **4.5: Report** (1 point)\n",
    "* **Question 5: Improve the accuracy** (BONUS) *(5 points)*\n",
    "  * **5.1: Custom model** (3 points)\n",
    "  * **5.2: Pretrained Network** (2 points)\n",
    "\n",
    "**TOTAL POINTS ARE 35, MAXIMUM GRADE IS 30**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b40d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    %pip install -qqq numpy scipy matplotlib pandas scikit-learn seaborn tqdm torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960c7ef",
   "metadata": {
    "id": "skGOzNBb3uF4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # imports a fast numerical programming library\n",
    "import matplotlib.pyplot as plt # sets up plotting under plt\n",
    "import pandas as pd # lets us handle data as dataframes\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sets up pandas table display\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c56914",
   "metadata": {
    "id": "_R7KkUpP3uF4"
   },
   "source": [
    "**Notation:**\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features\n",
    "\n",
    "**Let's start by setting up our Python environment and importing the required libraries:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306465a9",
   "metadata": {
    "id": "0G_LPstI3uF6",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1: **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37beb8",
   "metadata": {
    "id": "ffcP6hxn3uF7"
   },
   "source": [
    "### **1.1: Log-likelihood and Gradient Ascent Rule** \n",
    "\n",
    "Write the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
    "\n",
    "Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
    "\n",
    "Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c4718",
   "metadata": {
    "id": "kWGef1atE3-I"
   },
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "**WRITE YOUR EQUATIONS HERE**\n",
    "\n",
    "- **Likelihood**: \n",
    "\\begin{align}\n",
    "L(\\theta) &= \\prod_{i=1}^m \\big(h_\\theta(x_i)\\big)^{y_i} \\big(1 - h_\\theta(x_i)\\big)^{1-y_i}\n",
    "\\end{align}\n",
    "\n",
    "- **Log-Likelihood**: \n",
    "\\begin{align}\n",
    "l(\\theta) &= \\sum_{i=1}^m \\Big[ y_i \\ln h_\\theta(x_i) + (1 - y_i) \\ln \\big(1 - h_\\theta(x_i)\\big) \\Big]\n",
    "\\end{align}\n",
    "\n",
    "- **Gradient of log-likelihood** (slide 5 p. 20):\n",
    "\\begin{align}\n",
    "\\frac{\\delta l(\\theta)}{\\delta \\theta_j} &= \\sum_{i=1}^m \\Big[ y_i - h_\\theta(x_i) \\Big] x_{ij}\n",
    "\\end{align}\n",
    "\n",
    "- **Gradient update equation**: \n",
    "For  $j=0,...,n$:\n",
    "\\begin{equation}\n",
    "\\theta_j := \\theta_j + \\alpha \\sum_{i=1}^m \\Big[ y_i - h_\\theta(x_i) \\Big] x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69909e04",
   "metadata": {
    "id": "4N20uGxT3uGA"
   },
   "source": [
    "### **1.2: Logistic regression with Gradient Ascent**\n",
    "\n",
    "Define the sigmoid function `sigmoid`, then define the `LogisticRegression` class with the relative methods necessary to make predictions on an input, compute the log-likelihood and update its parameters. \n",
    "Then define a function that takes in input such $X$, $y$ and the predictions $\\hat{y} = g(\\theta^{T}x)$ and computes the gradient of the log-likelihood.\n",
    "Finally implement a function that takes in input such class and performs the training loop with the specified hyperparameters.\n",
    "\n",
    "Translate the equations you wrote above in code to learn the logistic regression parameters, $x^{(i)}_1$ and $x^{(i)}_2$ represent the two features for the $i$-th data sample $x^{(i)}$ and $y^{(i)}$ is its ground truth label.\n",
    "\n",
    "*Hint: even though by definition log likelihood and gradient ascent are defined by summations, for numerical stability it is advised to use the mean operation.*\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/logisic_regression.py`, `libs/math.py/sigmoid()` and `libs/optim.py`**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4829929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.models import LogisticRegression\n",
    "from libs.optim import fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca378a",
   "metadata": {
    "id": "c2q2DZXF3uGB"
   },
   "source": [
    "**Check your grad_l implementation:**\n",
    "\n",
    "`LogisticRegression.log_likelihood` applied to some random vectors should provide a value for `output_test` close to the `target_value` (defined below).\n",
    "In other words, `error_test` should be close to 0.\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df146f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3UT5wav3uGB",
    "outputId": "e71fbb2c-8abf-4cf8-e927-23fef4a95dde"
   },
   "outputs": [],
   "source": [
    "target_value = -1\n",
    "np.random.seed(1)\n",
    "output_test = LogisticRegression.likelihood(np.random.random(100), np.random.randint(0, 2, 100))\n",
    "error_test = np.abs(output_test - target_value)\n",
    "print(\"Error: \", error_test)\n",
    "assert error_test < 0.2, \"The output is not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876a9e2",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=800/>\n",
    "\n",
    "Now you'll load a dataset of penguins data. The dataset contains three species of penguins (Adelie, Gentoo and Chinstrap). Your goal will be to classify a penguin species based on their bill's length and body mass. First we'll load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"assets/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee1a83",
   "metadata": {},
   "source": [
    "We want to train a classifier capable of understanding the difference between Adelie and Gentoo solely based on their bill's length and body mass. Thus in order to preprocess the data we:\n",
    "1. Drop all the items with null data.\n",
    "2. Remove the third species (Chinstrap) from the dataset.\n",
    "3. Select the features we're interested in (`bill_length`, `body_mass`).\n",
    "4. Select the label data and encode it in the values 0 and 1.\n",
    "\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png\" width=\"500\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data = data[data[\"species\"] != \"Chinstrap\"]\n",
    "X = data[[\"bill_length\", \"body_mass\"]]\n",
    "y = data[\"species\"].map({\"Adelie\": 0, \"Gentoo\": 1}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cb402",
   "metadata": {},
   "source": [
    "It is recommended to normalize data when using machine learning techniques, so now normalize $X$ to have $\\mu=0, \\sigma=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057099fb",
   "metadata": {},
   "source": [
    "We add a column of 1's to $X$ to take into account the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118726ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"bias\"] = 1\n",
    "# Reordering columns to have the bias term first (convention)\n",
    "X = X[[\"bias\", \"bill_length\", \"body_mass\"]] \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6c591",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Now you'll use the class defined above to train a logistic regression model on classifying a group of penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70727f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajh8uvxR3uGB",
    "outputId": "a3ea1017-fd6e-4e2c-83bb-e337cc9b2ef0"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(num_features=X.shape[1])\n",
    "\n",
    "# Run Gradient Ascent method\n",
    "n_iter = 50\n",
    "log_l_history, _ = fit(model, X, y, lr=0.5, num_steps=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ba43d",
   "metadata": {
    "id": "MusdHuGZ3uGC"
   },
   "source": [
    "Let's plot the log likelihood over different iterations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b2578",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5BFYiF543uGC",
    "outputId": "7cf633f9-3c86-472f-9954-350acc5a5fb3"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(log_l_history)), log_l_history, \"b\")\n",
    "plt.ylabel(\"log-likelihood(Theta)\")\n",
    "plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333bcd8",
   "metadata": {
    "id": "pYd890o33uGC"
   },
   "source": [
    "Plot the data and the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(data=X, x=\"bill_length\", y=\"body_mass\", hue=data[\"species\"])\n",
    "\n",
    "x_range = np.linspace(X['bill_length'].min(), X['bill_length'].max(), 100)\n",
    "theta_final = model.parameters\n",
    "y_range = -(theta_final[0] + theta_final[1] * x_range) / theta_final[2]\n",
    "plt.plot(x_range, y_range, c=\"red\")\n",
    "\n",
    "plt.xlim(X['bill_length'].min() - 0.2, X['bill_length'].max() + 0.2)\n",
    "plt.ylim(X['body_mass'].min() - 0.2, X['body_mass'].max() + 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = ((model.predict(X) > 0.5) == y).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "assert accuracy > 0.6, \"The accuracy is too low\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad244e",
   "metadata": {
    "id": "FgyCuwL3E3-P"
   },
   "source": [
    "### **1.3: Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422a9aa",
   "metadata": {
    "id": "YyHpS-us3uGD"
   },
   "source": [
    "1. Are we looking for a local minimum or a local maximum using the gradient ascent rule? \n",
    "2. You have implemented the gradient ascent rule. Could we have also used gradient descent instead for the proposed problem? Why/Why not?\n",
    "3. Let's deeply analyze how the learning rate $\\alpha$ and the number of iterations affect the final results. Run the algorithm you have written for different values of $\\alpha$ and the number of iterations and look at the outputs you get. Is the decision boundary influenced by these parameters change? Why do you think these parameters are affecting/not affecting the results?\n",
    "4. What happens if you do not normalize the data? Try to run the algorithm without normalizing the data and see what happens. Why do you think this happens?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87caf56",
   "metadata": {
    "id": "4tHm2tj5E3-P"
   },
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "1. Gradient ascent is a technique used when we want to maximize a function, meaning we aim to find a local maximum. On the other hand, gradient descent is its opposite‚Äîit's used to minimize a function, helping us find a local minimum. In machine learning, gradient ascent is particularly useful in cases like maximizing the log-likelihood in probabilistic models or optimizing reward functions in reinforcement learning tasks.\n",
    "2. Interestingly, gradient descent can achieve the same result if we rewrite the problem as minimizing the negative of the function. This works because minimizing \n",
    "‚àí\n",
    "ùëì\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "‚àíf(x) essentially follows the same path as maximizing \n",
    "ùëì\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "f(x), leading to the same solution in the end.\n",
    "3. The learning rate (alpha) and the number of iterations are key factors in optimization. A higher learning rate can speed things up but might overshoot the optimal solution, causing instability, while a very small learning rate ensures stability but makes progress much slower and risks getting stuck in less-than-ideal solutions. Similarly, increasing the number of iterations gives the algorithm more opportunities to find the best solution, but after a certain point, the improvements become minimal. In some cases, too many iterations can even lead to overfitting. For example, running a gradient ascent algorithm with different learning rates, like 0.01, 0.1, or 1.0, and varying iterations, such as 50, 100, or 500, shows that a small learning rate gives stable but slow results, while a very high one might cause the process to diverge or oscillate. The number of iterations helps refine the solution, but after a certain point, the changes in the outcome are barely noticeable.\n",
    "4. When data isn‚Äôt normalized, features with larger values can overpower smaller ones, leading to uneven parameter updates and a distorted or less effective decision boundary. Normalizing the data levels the playing field, ensuring all features contribute equally and allowing the algorithm to learn more effectively. Without normalization, the optimization process can become slower, less stable, and more likely to get stuck in local optima, ultimately harming the model‚Äôs ability to generalize to new data. Testing the algorithm with and without normalization clearly shows the difference, with unnormalized data often resulting in poorer performance, unstable outcomes, and slower progress.\n",
    "5. *(feel free to add here screenshots or new code cells if needed)*\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ad46b",
   "metadata": {
    "id": "vbdZNYCl3uGD",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2: **Polynomial Expansion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022266f7",
   "metadata": {
    "id": "TrcB4LXw3uGD",
    "tags": []
   },
   "source": [
    "### **2.1: Polynomial features for logistic regression** \n",
    "\n",
    "Define new features e.g., of 2nd and 3rd degree, and learn a logistic regression classifier by using the new features and the gradient ascent optimization algorithm defined in Question 1.\n",
    "\n",
    "In particular, consider a polynomial boundary with equation:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2\n",
    "\\end{equation}\n",
    "\n",
    "Therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.\n",
    "\n",
    "Create new arrays by stacking $x$ and the new 7 features (in the order $x_1x_1, x_2x_2, x_1x_2, x_1x_1x_1, x_2x_2x_2, x_1x_1x_2, x_1x_2x_2$). \n",
    "In particular create `x_new_quad` by additionally stacking $x$ with the quadratic features, and `x_new_cubic` by additionally stacking $x$ with the quadratic and the cubic features.\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200aed3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXn0rvSM3uGD",
    "outputId": "09a90b76-0b2e-4b6c-c75f-8df4dd673954"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(\n",
    "    n_samples=700,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=.3,\n",
    "    random_state=89,\n",
    ")\n",
    "X = np.hstack([np.ones_like(X[:, [0]]), X])\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=200, random_state=42)\n",
    "\n",
    "sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc0176",
   "metadata": {
    "id": "zicVdhc73uGF"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def get_polynomial(X, degree):\n",
    "    \"\"\"\n",
    "    Given an initial set of features, this function computes the polynomial features up to the given degree.\n",
    "\n",
    "    Args:\n",
    "        X: the initial features matrix of shape (n_samples, 3) where the first column is the bias term\n",
    "        degree: the degree of the polynomial\n",
    "\n",
    "    Returns:YOUR\n",
    "        X: the final polynomial features\n",
    "    \"\"\"\n",
    "    if degree < 2:\n",
    "        return X\n",
    "\n",
    "    # Start with a column of ones for the bias term.\n",
    "    features = np.ones((X.shape[0], 1))\n",
    "\n",
    "    # Loop through all combinations of features for the given degree.\n",
    "    # Iterate over degrees 1 through `degree`\n",
    "    for d in range(1, degree + 1):\n",
    "        # Use combinations with replacement for feature indices\n",
    "        for comb in combinations_with_replacement(range(1, X.shape[1]), d):\n",
    "            # Compute the product of the features for the current combination\n",
    "            feature_comb = np.prod([X[:, idx] for idx in comb], axis=0)\n",
    "            features = np.column_stack([features, feature_comb])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca7ebf",
   "metadata": {
    "id": "vzyJ450Z3uGF"
   },
   "source": [
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a2aee",
   "metadata": {
    "id": "D7pukmkA3uGF"
   },
   "outputs": [],
   "source": [
    "x_new_quad = get_polynomial(X, degree=2)\n",
    "x_new_cubic = get_polynomial(X, degree=3)\n",
    "print(x_new_quad.shape, x_new_cubic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a43fe0",
   "metadata": {
    "id": "iFlwv5JY3uGF"
   },
   "source": [
    "Now use the gradient ascent optimization algorithm to learn the models by maximizing the log-likelihood, both for the case of `x_new_quad` and `x_new_cubic`.\n",
    "\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea40537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFTkk32y3uGF",
    "outputId": "1177abc2-7d88-423e-f7c6-07e29293abfa"
   },
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "model_lin = LogisticRegression(num_features=X.shape[1])\n",
    "log_l_history,_ = fit(model_lin, X, y, lr=0.5, num_steps=n_iter)\n",
    "\n",
    "# Initialize model, in case of quadratic features\n",
    "model_quad = LogisticRegression(num_features=x_new_quad.shape[1])\n",
    "log_l_history_quad,_ = fit(model_quad, x_new_quad, y, lr=0.5, num_steps=n_iter)\n",
    "\n",
    "# Initialize model, in case of quadratic and cubic features\n",
    "model_cubic = LogisticRegression(num_features=x_new_cubic.shape[1])\n",
    "log_l_history_cubic,_ = fit(model_cubic, x_new_cubic, y, lr=0.5, num_steps=n_iter)\n",
    "\n",
    "log_l = np.stack([log_l_history, log_l_history_quad, log_l_history_cubic])\n",
    "\n",
    "log_l_df = pd.DataFrame(log_l.T, columns=[\"Linear\", \"Quadratic\", \"Cubic\"])\n",
    "sns.lineplot(data=log_l_df, markers=True).set(\n",
    "    xlabel=\"Iterations\", ylabel=\"Log Likelihood\", title=\"Log Likelihood History for Different Models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60667ee",
   "metadata": {
    "id": "Zy_2fRVP3uGG"
   },
   "source": [
    "### **2.2: Plot the computed non-linear boundary** \n",
    "\n",
    "First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcebc714",
   "metadata": {
    "id": "fd4r2Z3z3uGG"
   },
   "outputs": [],
   "source": [
    "def boundary_function(x1_vec, x2_vec, theta_final, degree):\n",
    "    \"\"\"\n",
    "    This function computes the boundary function for the given theta_final and degree.\n",
    "\n",
    "    Args:\n",
    "        x1_vec: the x1 vector\n",
    "        x2_vec: the x2 vector\n",
    "        theta_final: the final theta\n",
    "        degree: the degree of the polynomial\n",
    "\n",
    "    Returns:\n",
    "        x1_vec: the x1 vector\n",
    "        x2_vec: the x2 vector\n",
    "        f: the boundary function\n",
    "    \"\"\"\n",
    "\n",
    "    x1_vec, x2_vec = np.meshgrid(x1_vec, x2_vec)\n",
    "    \n",
    "    # Prepare input for polynomial features generation\n",
    "    X = np.column_stack([np.ones_like(x1_vec.ravel()), x1_vec.ravel(), x2_vec.ravel()])\n",
    "    \n",
    "    # Generate polynomial features\n",
    "    X_poly = get_polynomial(X, degree)\n",
    "    \n",
    "    # Compute boundary function\n",
    "    f = np.dot(X_poly, theta_final)\n",
    "    \n",
    "    return x1_vec, x2_vec, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b556291",
   "metadata": {
    "id": "n2udd0d63uGG"
   },
   "source": [
    "Now plot the decision boundaries corresponding to the theta_final_quad and theta_final_cubic solutions.\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary_function(\n",
    "    X: np.ndarray, y: np.ndarray, theta: np.ndarray, degree: int, n_points: int = 200\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function plots the boundary function for the given theta and degree.\n",
    "\n",
    "    Args:\n",
    "        X: the input data\n",
    "        y: the input labels\n",
    "        theta: the final theta\n",
    "        degree: the degree of the polynomial\n",
    "        n_points: the number of points to plot\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    x1_vec = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, n_points)\n",
    "    x2_vec = np.linspace(X[:, 2].min() - 1, X[:, 2].max() + 1, n_points)\n",
    "\n",
    "    x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta, degree=degree)\n",
    "    mesh_shape = int(np.sqrt(f.shape[0]))\n",
    "\n",
    "    sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y, legend=False)\n",
    "    plt.contour(\n",
    "        x1_vec, x2_vec, f.reshape((mesh_shape, mesh_shape)), colors=\"red\", levels=[0]\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plot_boundary_function(X, y, model_lin.parameters, degree=1)\n",
    "plt.title(\"Decision Boundary for Quadratic Features\")\n",
    "plt.subplot(1,3,2)\n",
    "plot_boundary_function(X, y, model_quad.parameters, degree=2)\n",
    "plt.title(\"Decision Boundary for Quadratic Features\")\n",
    "plt.subplot(1,3,3)\n",
    "plot_boundary_function(X, y, model_cubic.parameters, degree=3)\n",
    "plt.title(\"Decision Boundary for Cubic Features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12908fe6",
   "metadata": {},
   "source": [
    "**Polynomial degree and overfitting**\n",
    "\n",
    "As the polynomial degree increases, the decision boundary becomes more and more complex. This can lead to overfitting, i.e. the model learns the training data too well, and it is not able to generalize to new data. This is a common problem in machine learning, and it is important to be able to detect it.\n",
    "\n",
    "In order to detect overfitting, we can split the dataset into a training set and a test set. The training set is used to learn the model, while the test set is used to evaluate the model performance on new data. If the model performs well on the training set, but it performs poorly on the test set, then we have overfitting.\n",
    "\n",
    "In this exercise, you are asked to plot the training and test accuracy as a function of the polynomial degree. Consider all the polynomial degrees from 1 to 20. For each polynomial degree, learn the model on the training set, and evaluate the accuracy on both the training and the test set. Additionally, visualize the decision boundary for the polynomials that give the **best** and the **worst** test accuracy for $\\texttt{degree} \\geq 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def fit_polynomials(X, y, X_test, y_test, degrees, lr, num_steps, architecture = LogisticRegression):\n",
    "    \"\"\"\n",
    "    This function fits a logistic regression model for each degree in the degrees list.\n",
    "    \"\"\"\n",
    "    X = normalize(X)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    thetas = []\n",
    "    accuracy_scores_train, accuracy_scores_test = [], []\n",
    "    for degree in tqdm(degrees):\n",
    "        x_new = get_polynomial(X, degree=degree)\n",
    "\n",
    "        model = architecture(num_features=x_new.shape[1])\n",
    "        fit(model, x_new, y, lr=lr, num_steps=num_steps)    \n",
    "\n",
    "        thetas.append(model.parameters)\n",
    "        y_hat_train = model.predict(x_new) > 0.5\n",
    "        accuracy_scores_train.append(accuracy_score(y, y_hat_train))\n",
    "        y_hat_test = model.predict(get_polynomial(X_test, degree=degree)) > 0.5\n",
    "        accuracy_scores_test.append(accuracy_score(y_test, y_hat_test))\n",
    "        \n",
    "    return thetas, accuracy_scores_train, accuracy_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28971c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(1, 20)\n",
    "np.random.seed(42)\n",
    "thetas, accuracy_scores_train, accuracy_scores_test = fit_polynomials(\n",
    "    X, y, X_val, y_val, degrees=degrees, lr=0.5, num_steps=500, architecture=LogisticRegression\n",
    ")\n",
    "sns.lineplot(x=degrees, y=accuracy_scores_train, label=\"Train\")\n",
    "sns.lineplot(x=degrees, y=accuracy_scores_test,  label=\"Test\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xticks(degrees)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0052d",
   "metadata": {},
   "source": [
    "Plot the best and the worst decision boundaries for $\\texttt{degree} \\geq 2$.\n",
    "\n",
    "--------------------------------------------\n",
    "**Write your code below this line**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c483b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot worst model\n",
    "accuracy_scores_test_without_one = accuracy_scores_test[1:]\n",
    "\n",
    "pos = np.argmin(accuracy_scores_test_without_one)\n",
    "worst_degree = degrees[pos+1]\n",
    "worst_thetas = thetas[pos+1]\n",
    "\n",
    "\n",
    "# Create a figure for displaying the best and worst decision boundaries\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Plot the best decision boundary (for the best degree)\n",
    "plot_boundary_function(X, y, worst_thetas, degree=worst_degree)\n",
    "plt.title(f\"Worst Decision Boundary for Degree {worst_degree}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa71ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best model\n",
    "pos = np.argmax(accuracy_scores_test_without_one)\n",
    "best_degree = degrees[pos+1]\n",
    "best_thetas = thetas[pos+1]\n",
    "\n",
    "\n",
    "# Create a figure for displaying the best and worst decision boundaries\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Plot the best decision boundary (for the best degree)\n",
    "plot_boundary_function(X, y, best_thetas, degree=best_degree)\n",
    "plt.title(f\"Best Decision Boundary for Degree {best_degree}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eae9e4",
   "metadata": {
    "id": "rQGJhknOE3-U"
   },
   "source": [
    "#### **Report** \n",
    "Write now your considerations. Discuss in particular:\n",
    "1. Look back at the plots you have generated. What can you say about the differences between the linear, quadratic, and cubic decision boundaries? Can you say if the model is improving in performances, increasing the degree of the polynomial? Do you think you can incur in underfitting increasing more and more the degree?\n",
    "2. Look at the plot of the training and test accuracy as a function of the polynomial degree. What can you say about the differences between the training and test accuracy? What can you say about the differences between the best and the worst test accuracy? In general, is it desirable to have a very complex decision boundary, i.e. a very high degree of the polynomial? Discuss and motivate your answer. \n",
    "3. In general what are some properties of the dataset that makes it more prone to overfitting? Discuss their impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abefa0",
   "metadata": {
    "id": "9C3gOx5_E3-U"
   },
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "1. The evolution of decision boundaries across polynomial degrees shows a clear progression in complexity and fitting capability. The linear boundary demonstrates significant limitations, creating straight lines that fail to capture the natural separation between classes. Moving to quadratic boundaries marks a substantial improvement, as the curved lines better adapt to the data's natural distribution. The cubic boundary introduces even more complex curves, though this added complexity might exceed what's necessary for effective classification, so increasing the polynomial degree will not lead to underfitting, but instead to overfitting. The accuracy plot demonstrates that the model's performance improves significantly from degree 1 to 2, but then stabilizes, suggesting that increasing polynomial degree beyond this point doesn't yield meaningful improvements. This indicates that degree 2 provides an optimal balance between model complexity and performance for this dataset.\n",
    "\n",
    "2. The relationship between training and test accuracy provides valuable insights into model behavior. After degree 2, the test accuracy stabilizes around 85%, while training accuracy maintains a consistently lower level around 80%. This stable gap between training and test performance shows the classic pattern where training accuracy exceeds test accuracy, indicating that while the model learns from the training data, it maintains reasonable generalization without severe overfitting. Having a very complex decision boundary (high polynomial degree) is generally undesirable for several reasons: it increases computational complexity, risks overfitting the data, doesn't provide significant performance improvements, and potentially reduces the model's generalization capability. The evidence suggests that simpler models, particularly those using quadratic features, achieve optimal performance while maintaining good generalization ability. The stability of both training and test accuracy after degree 2-3 further supports this conclusion.\n",
    "\n",
    "3. Several dataset characteristics can make a dataset particularly susceptible to overfitting. Limited dataset size makes it difficult for models to distinguish between genuine patterns and random noise. High noise levels in the data can cause models to fit irrelevant variations rather than true underlying patterns. High dimensionality relative to the number of samples increases the risk of finding spurious correlations. Class imbalance can lead to biased model behavior, while outliers can force models to create unnecessarily complex decision boundaries to accommodate these extreme points. Sparse data regions can lead to unreliable boundary decisions due to insufficient information. In the given dataset, we can observe several of these characteristics: overlapping classes, presence of outliers, varying data density across different regions, and noticeable noise in the distribution. These properties explain why a moderate complexity model (quadratic) performs well enough, and why increasing complexity further doesn't provide substantial benefits. This scenario illustrates the importance of matching model complexity to the inherent structure and challenges present in the data.\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456b6e",
   "metadata": {},
   "source": [
    "### **2.4: Weight Penalization**\n",
    "\n",
    "Look at how complicated the decision boundaries become as you increase the degree. Can we improve this and prevent overfitting?\n",
    "When dealing with overfitting one frequent solution is to use a weigth penalization technique like L2 or L1 penalization. \n",
    "\n",
    "In our case we'll use L2 regularization. In this way the regularized likelihood will be:\n",
    "$$\n",
    "\\texttt{Likelihood}_{reg}(\\theta) = \\texttt{Likelihood}(\\theta) - \\frac{\\lambda}{2n} \\sum^n_i \\theta_i^2\n",
    "$$\n",
    "Thus we can derive the update rule as:\n",
    "\\begin{equation}\n",
    "\\theta_j:= \\theta_j + \\alpha( \\frac{\\partial l(\\theta_j)}{\\partial \\theta_j} -  \\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{\\lambda}{2} \\theta_j^2 \\right ) )\n",
    "\\end{equation}\n",
    "\n",
    "Calculating the second term of the update rule it's just a matter of analytically solving a simple gradient, do it, and then implement it by extending the `LogisticRegression` class:\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/logisic_regression_penalized.py`**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.models import LogisticRegressionPenalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19371cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(\n",
    "    n_samples=500, \n",
    "    n_features=100, \n",
    "    n_informative=50, \n",
    "    n_redundant=25, \n",
    "    n_classes=2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "np.random.seed(42)\n",
    "\n",
    "lr = LogisticRegression(X.shape[1])\n",
    "likelihood_history, val_loss_history = fit(lr, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
    "\n",
    "penalized_lt = LogisticRegressionPenalized(X.shape[1], 2)\n",
    "pen_history, pen_val_history = fit(penalized_lt, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(-likelihood_history[2:], label=\"Train\", color=\"violet\")\n",
    "plt.plot(val_loss_history[2:], label=\"Test\", color='teal')\n",
    "plt.plot(-pen_history[2:], label=\"Train - penalized\", color=\"violet\", linestyle=\"--\")\n",
    "plt.plot(pen_val_history[2:], label=\"Test - penalized\", color=\"teal\", linestyle=\"--\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Non-penalized\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bee8a3",
   "metadata": {},
   "source": [
    "Now, evaluate the Penalized Logistic Regression for each value of $\\lambda \\in [0,3]$ and find the one that performs the best: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(0, 3, 0.1)\n",
    "losses = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    # Initialize Penalized Logistic Regression with current lambda\n",
    "    model = LogisticRegressionPenalized(num_features=X.shape[1], lambda_=lambda_)\n",
    "    \n",
    "    # Train the model\n",
    "    pen_history, pen_val_history = fit(model, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
    "    \n",
    "    # Compute validation loss\n",
    "    preds_val = model.predict(X_val)\n",
    "    validation_loss = -model.likelihood(preds_val, y_val)  # Negative log-likelihood\n",
    "    losses.append(validation_loss)\n",
    "# Plot the validation loss against lambda values\n",
    "if len(losses) > 0:\n",
    "    sns.lineplot(x=lambdas, y=losses, label=\"Validation Loss\").set(\n",
    "        xlabel=\"Lambda\", ylabel=\"Loss\", title=\"Validation Loss vs Lambda\"\n",
    "    )\n",
    "    print(f\"Best lambda: {lambdas[np.argmin(losses)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a196b",
   "metadata": {},
   "source": [
    "#### Report\n",
    "Write now your considerations. In particular:\n",
    "1. What happens when we use a non-penalized logistic regression?\n",
    "2. Observe the plot of the Train and Validation losses in the penalized vs non penalized case. In which case is the Train loss better? Can you explain why?\n",
    "3. What is the convergence rate? How is it influenced by the penalization?\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "1. When we use a non-penalized logistic regression, the loss computed on the validation set is higher than in the penalized cases. This is due to **overfitting**, as the non-penalized model is free to assign large weights to features, even if they are noisy or less informative. While this reduces the training loss, it makes the model less generalizable to unseen data, leading to higher validation loss.\n",
    "2. The **train loss** is lower in the non-penalized logistic regression compared to the penalized case. This happens because the penalization (L2 regularization) adds a constraint that discourages large weights, which slightly sacrifices training accuracy. However, the penalized logistic regression achieves better validation loss because regularization reduces overfitting, helping the model generalize better to unseen data. This trade-off demonstrates the benefit of regularization in achieving a balance between fitting the training data and maintaining simplicity for better generalization.\n",
    "3.  The **convergence rate** of the algorithm is slightly slower in the penalized logistic regression compared to the non-penalized case. This is because the gradient update rule in the penalized model includes an additional term ($\\lambda \\theta $) to account for the regularization penalty, which adjusts the weights more conservatively. Higher values of $ \\lambda $ further slow down convergence since the penalty term dominates the gradient. However, the slower convergence is often a worthwhile trade-off as it prevents the model from overfitting and leads to better validation performance.\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faba031",
   "metadata": {
    "id": "JTeuyG-S3uGH",
    "tags": []
   },
   "source": [
    "## 3: **Multinomial Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107612c",
   "metadata": {
    "id": "KS-AS1PUE3-V"
   },
   "source": [
    "### **3.1: Softmax Regression Model**\n",
    "\n",
    "In the multinomial classification we generally have $K>2$ classes. So the label for the $i$-th sample $X_i$ is $y_i\\in\\{1,...,K\\}$, where $i=1,...,N$. The output class for each sample is estimated by returning a score $s_i$ for each of the K classes. This results in a vector of scores of dimension K. \n",
    "In this exercise we'll use the *Softmax Regression* model, which is the natural extension of *Logistic Regression* for the case of more than 2 classes. The score array is given by the linear model:\n",
    "\n",
    "\\begin{align*}\n",
    "s_i =  X_i \\theta\n",
    "\\end{align*}\n",
    "\n",
    "Scores may be interpreted probabilistically, upon application of the function *softmax*. The position in the vector with the highest probability will be predicted as the output class. The probability of the class k for the $i$-th data sample is:\n",
    "\n",
    "\\begin{align*}\n",
    "p_{ik} = \\frac{\\exp(X_i \\theta_k)}{\\sum_{j=1}^K(X_i \\theta_j))}\n",
    "\\end{align*}\n",
    "\n",
    "We will adopt the *Cross Entropy* loss and optimize the model via *Gradient Descent*. \n",
    "In the first of this exercise we have to: \n",
    "-    Write the equations of the Cross Entropy loss for the Softmax regression model;\n",
    "-    Compute the equation for the gradient of the Cross Entropy loss for the model, in order to use it in the gradient descent algorithm.\n",
    "\n",
    "#### A bit of notation\n",
    "\n",
    "*  N: is the number of samples \n",
    "*  K: is the number of classes\n",
    "*  X: is the input dataset and it has shape (N, H) where H is the number of features\n",
    "*  y: is the output array with the labels; it has shape (N, 1)\n",
    "*  $\\theta$: is the parameter matrix of the model; it has shape (H, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c8304",
   "metadata": {
    "id": "xHX1s7jp3uGI"
   },
   "source": [
    "--------------------------------------------\n",
    "**Write you equation below this line**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10170a74",
   "metadata": {
    "id": "ixObV4w43uGI"
   },
   "source": [
    "\\begin{align*}\n",
    "L(\\theta) = \\prod_{i=1}^N \\prod_{k=1}^K p_{ik}^{y_{ik}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "Loss(\\theta) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^K y_{ik} \\log(p_{ik})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta_k} L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N X_i \\left( p_{ik} - y_{ik} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "- $ y_{ik} $ is the one-hot encoded label for the $i$-th sample and $k$-th class. It equals 1 if the true class of the $i$-th sample is $k$, and 0 otherwise.\n",
    "- $p_{ik}$ is the predicted probability for class $k$ of the $i$-th sample, computed using the softmax function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccc0db",
   "metadata": {
    "id": "rMxrcWc53uGI"
   },
   "source": [
    "### **3.2: Coding**\n",
    "\n",
    "We are using the CIFAR-10 dataset for this exercise. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It has 50,000 training images and 10,000 test images. The dataset was established by the Canadian Institute For Advanced Research (CIFAR), and it has become a standard benchmark for machine learning algorithms, especially in the area of image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e56f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "cifar_dir = \"assets/cifar10\"\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=cifar_dir, train=True, download=True, transform=transform\n",
    ")\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=cifar_dir, train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    encoded = np.zeros((len(y), num_classes))\n",
    "    for i, val in enumerate(y):\n",
    "        encoded[i, val] = 1\n",
    "    return encoded\n",
    "\n",
    "# Evaluate the accuracy of the predictions\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    correct_predictions = np.sum(predictions == true_labels)\n",
    "    total_predictions = len(true_labels)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = [img.reshape(-1).numpy() for img, _ in train_data]\n",
    "X_train = np.array(X_train)\n",
    "y_train = [label for _, label in train_data]\n",
    "\n",
    "X_val = [img.reshape(-1).numpy() for img, _ in test_data]\n",
    "X_val = np.array(X_val)\n",
    "y_val = [label for _, label in test_data]\n",
    "\n",
    "\n",
    "# Add bias term to X\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "\n",
    "y_train_onehot = one_hot_encode(y_train)\n",
    "y_test_onehot = one_hot_encode(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f30d9",
   "metadata": {
    "id": "2RrCmafP3uGI"
   },
   "source": [
    "*Hint: consider the labels as one-hot vector. This will allow matrix operations (element-wise multiplication and summation).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63efe6",
   "metadata": {},
   "source": [
    "Now implement a classifier for Multinomial Classification using the `softmax` function. Again, implement it as a class with the methods:\n",
    "- `predict`\n",
    "- `predict_labels`\n",
    "- `likelihood` *(Here you need to implement the Cross Entropy Loss)*\n",
    "- `update_theta`\n",
    "- `compute_gradient` to compute the Jacobian $\\nabla$\n",
    "\n",
    "Note that this this you don't need to reimplement the `fit()` function since the training loop you defined above works also for a Multinomial Classifier, provided that this is structured with the previously mentioned methods.\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/multinomial.py` and `libs/math.py/softmax()`**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbca091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.models import SoftmaxClassifier\n",
    "from libs.optim import fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21abf9",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "**Do not write below this line just run it**\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "*Execution can take around 10 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply gradient descent to optimize theta\n",
    "alpha = 0.01\n",
    "iterations = 500\n",
    "H, K = X_train.shape[1], 10  # number of features and number of classes\n",
    "model = SoftmaxClassifier(num_features=H, num_classes=K)\n",
    "loss_history, _ = fit(model, X_train, y_train_onehot, lr=alpha, num_steps=iterations)\n",
    "\n",
    "# Make predictions on the training and test data\n",
    "train_predictions = model.predict_labels(X_train)\n",
    "test_predictions = model.predict_labels(X_val)\n",
    "\n",
    "train_accuracy = compute_accuracy(train_predictions, y_train)\n",
    "test_accuracy = compute_accuracy(test_predictions, y_val)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46307d23",
   "metadata": {},
   "source": [
    "### **3.3: Pipeline**\n",
    "Now you're going to use `scikit-learn` library to build a pipeline of operations to redo everything we've done so far in the homework. First we have loaded the required modules and the penguins dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Then here you'll build the pipeline. We need four items:\n",
    "1. The Numerical Transformer, to handle the preprocessing of numerical columns, by:\n",
    "    - Imputing missing values with their mean\n",
    "    - Enrich the features with a 3-rd degree polynomial expansion\n",
    "    - Scaling of the features to $\\mu=0, \\sigma=1$\n",
    "2. The Categorical Transformer, to handle the preprocessing of categorical values, by:\n",
    "    - Imputing the missing values with the most frequent value\n",
    "    - Encode the features in a one-hot vector.\n",
    "3. The Preprocessor: a ColumnTransformer that distributed the numerical columns to the numerical transformer and the categorical columns to the categorical tranformer.\n",
    "4. The final Pipeline, which contains the preprocessor and the classfier of your choice (in this case `KNeighborsClassifier`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('assets/train.csv')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data.drop('species', axis=1), data.species, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['bill_length', 'bill_depth', 'flipper_length', 'body_mass']\n",
    "categorical_cols = ['island', 'sex']\n",
    "\n",
    "\n",
    "##############################################\n",
    "###          FILLED CODE                 #####\n",
    "##############################################\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy='mean'),PolynomialFeatures(degree=3, include_bias=False), StandardScaler())\n",
    "\n",
    "categoric_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),   \n",
    "        ('cat', categoric_transformer, categorical_cols)  \n",
    "    ],\n",
    "    remainder='drop'  \n",
    ")\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    KNeighborsClassifier(n_neighbors=10) # k for us is 10\n",
    ")\n",
    "if len(pipe.named_steps)>0:\n",
    "    display(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20cc4b",
   "metadata": {},
   "source": [
    "Now, we can use this pipeline to preprocess the input data and fit a classifier. Leveraging `scikit-learn`'s pipelines allows you to:\n",
    "- Define the entire chain of operations in a structured way, which is especially useful for cleaning and transforming data.\n",
    "- Separate the definition of operations from their execution, creating a clean and organized workflow.\n",
    "\n",
    "This approach makes it easier to manage complex preprocessing steps while maintaining readability and clarity in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0bc3d",
   "metadata": {},
   "source": [
    "### **3.4: Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd86ac",
   "metadata": {},
   "source": [
    "This is nice but can we improve it? In defining the pipeline you certainly used some fixed hyperparameters, for example the number of neighbors or the degree of the polynomial expansion.\n",
    "\n",
    "First, let's look at the list of hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcfd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = pipe.get_params()\n",
    "for hp, val in hparams.items():\n",
    "    if type(val) not in [int, float, str]:\n",
    "        continue\n",
    "    print(f\"{hp}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99eea0",
   "metadata": {},
   "source": [
    "Some of these hyperparameters are set to their default values, while others are explicitly defined. However, any data scientist knows that hyperparameters should not be arbitrarily chosen; instead, they should be optimized through **Cross-Validation**.\n",
    "\n",
    "We can leverage the compositionality of `scikit-learn` by incorporating the pipeline into a `GridSearchCV` class. This allows you to easily define a grid of parameters to val and automatically perform cross-validation over the combinations.\n",
    "\n",
    "Choose at least 2 values for at least 3 hyperparameters. val their impact on the model and find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {\n",
    "    'columntransformer__num__polynomialfeatures__degree': [2, 4], \n",
    "    'kneighborsclassifier__n_neighbors': [3, 5, 7],  \n",
    "    'kneighborsclassifier__weights': ['uniform', 'distance']  \n",
    "}\n",
    "\n",
    "pipe_cv = GridSearchCV(\n",
    "    pipe,  \n",
    "    param_grid=grid,  \n",
    "    cv=5,  \n",
    "    scoring='accuracy',  \n",
    "    verbose=1,  \n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "pipe_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best score: {pipe_cv.best_score_}\")\n",
    "for hp, val in pipe_cv.best_params_.items():\n",
    "    print(f\"{hp}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a24ba",
   "metadata": {},
   "source": [
    "`GridSearchCV` doesn't only find the best combination of hyperparmeters, but it also refits the model with the best hyperparameters it finds. Let's val this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76df863",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_cv.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a2f74",
   "metadata": {},
   "source": [
    "### **3.5 Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c4ac8",
   "metadata": {},
   "source": [
    "#### Report\n",
    "1. How many combinations has your gridsearch tried?\n",
    "2. Make a plot with the results of your hyperparameter grid\n",
    "3. Do you notice any trend in the performance of certain hyperparameters?\n",
    "4. Do the classifiers obtain the same accuracy on train and val sets? If not, try to give an explanation.\n",
    "5. With the choice of hyperparameters you made, do you notice any trade-off between accuracy and compute power? Show with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ce062",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(pipe_cv.cv_results_)\n",
    "cv_res.columns = [col.split('__')[-1] for col in cv_res.columns]\n",
    "\n",
    "##############################################\n",
    "###                CODE FILLED            ####\n",
    "##############################################\n",
    "\n",
    "#Total of combinations tried by GridSearchCV\n",
    "total_combinations = len(cv_res)\n",
    "print(f\"1. Total combinations tried by the gridsearch: {total_combinations}\")\n",
    "\n",
    "#Rsults\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv_res['polynomial_degree'] = cv_res['degree']\n",
    "cv_res['mean_accuracy'] = cv_res['mean_test_score']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=cv_res, x='n_neighbors', y='mean_accuracy', hue='weights', style='polynomial_degree', markers=True)\n",
    "plt.title(\"Hyperparameter Grid Search Results\")\n",
    "plt.xlabel(\"Number of Neighbors (n_neighbors)\")\n",
    "plt.ylabel(\"Mean Test Accuracy\")\n",
    "plt.legend(title=\"Weights / Degree\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Trends and analysis\n",
    "print(cv_res.groupby(['polynomial_degree', 'n_neighbors', 'weights'])['mean_accuracy'].mean())\n",
    "\n",
    "# Train vs. Validation Accuracy\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "y_pred_train = pipe_cv.predict(X_train)\n",
    "y_pred_val = pipe_cv.predict(X_val)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if train_accuracy > val_accuracy:\n",
    "    print(\"The model performs better on the training set, suggesting potential overfitting.\")\n",
    "elif train_accuracy < val_accuracy:\n",
    "    print(\"The model performs better on the validation set, which might indicate underfitting.\")\n",
    "else:\n",
    "    print(\"The model performs equally well on both sets.\")\n",
    "\n",
    "#Aaccuracy and compute power\n",
    "cv_res['fit_time'] = cv_res['mean_fit_time']\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(data=cv_res, x='fit_time', y='mean_accuracy', hue='polynomial_degree', style='weights')\n",
    "plt.title(\"Trade-off Between Accuracy and Compute Power\")\n",
    "plt.xlabel(\"Mean Fit Time (s)\")\n",
    "plt.ylabel(\"Mean Test Accuracy\")\n",
    "plt.legend(title=\"Degree / Weights\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e95b8",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "**Write your answer below this line**\n",
    "\n",
    "1. The total combinations tried by the gridsearch were 12\n",
    "2. The plot shows how the accuracychanges when there are different number of neighbors, wight (uniform, distance), and polynomial degrees. A degree of 2 tends to perform better than a degree of 4 across most combinations of hyperparameters.The best configuration in terms of mean_accuracy is polynomial gregree in 2, neighbors in 7, and weights = distance or uniform (both achieve mean_accuracy = 0.72).\n",
    "3. In general, lower polynomial degrees (simpler transformations) appear to generalize better. Also, increasing neighbors (n) tends to improve accuracy, suggesting that considering more neighbors provides more stable predictions for this dataset. However, the distance weighting scheme slightly outperforms uniform, particularly for higher n_neighbors.\n",
    "4. Given the train accuracy in 0.7578 and validation accuracy in 0.7317. The model performs better on the training set, suggesting potential overfitting. And, overfitting usually suggests that the model performs slightly worse on unseen data due to complexity or limited generalization.\n",
    "5. A higher degree increases the number of features and model complexity, leading to higher computation time.As seen in the plot, results for degree 4 (darker points) generally show lower or comparable accuracy to degree 2, despite the higher computational cost. In summary, There is a strong relation between higher polynomial degrees and larger n_neighbors that increase computational costs, but the improvement in accuracy depends on the configuration. Based on the results, degree at 2 with 7 neighbors  and weights equals to distance achieves a good balance between accuracy and compute power.\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a3c8e",
   "metadata": {
    "id": "OwYcUWZnrDgP"
   },
   "source": [
    "## **4: Debugging a CNN with Shape Errors**\n",
    "\n",
    "You are provided with a CNN model intended to classify images from the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. However, the model contains shape mismatches between layers due to intentional errors. Your first task is to identify and fix these errors to make the model functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d941ac13",
   "metadata": {
    "id": "9FPQbEr9rZIL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d671f",
   "metadata": {
    "id": "rWx5oYSet38n"
   },
   "source": [
    "### 4.1: Split the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881a353",
   "metadata": {
    "id": "IviQYVL0rbZ5"
   },
   "source": [
    "Instructions:\n",
    "\n",
    "* Define the Split Sizes:\n",
    "Calculate the sizes for the training and validation datasets. Allocate ***80% of the training*** data for the training set and *20% for the validation set*.\n",
    "\n",
    "* Split the Dataset:\n",
    "Use `torch.utils.data.random_split` to create the training and validation datasets from the original training dataset.\n",
    "\n",
    "* Create Data Loaders:\n",
    "Create data loaders for the training, validation, and test datasets using torch.utils.data.DataLoader with a ***batch size of 64***. Ensure that the training data is ***shuffled***.\n",
    "\n",
    "* Print the size of each dataset (train, test, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b82813",
   "metadata": {
    "id": "R3Tc8O6zrh6v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training set size: 40000\n",
      "Validation set size: 10000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for the data that we will use\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#####################################################\n",
    "##              YOUR CODE HERE                     ##\n",
    "#####################################################\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Define the sizes for training and validation datasets\n",
    "train_size = int(0.8 * len(full_train_dataset))  # 80% of the training data\n",
    "val_size = len(full_train_dataset) - train_size  # Remaining 20%\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders with batch size 64\n",
    "batch_size = 64\n",
    "\n",
    "# use these names for the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset sizes\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba0401",
   "metadata": {
    "id": "8WWMSLDSt9j7"
   },
   "source": [
    "### 4.2: Identify and Correct Errors in the CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806744d",
   "metadata": {
    "id": "Laxva2bSrodv"
   },
   "source": [
    "In this exercise, you will analyze an intentionally incorrect implementation of a Convolutional Neural Network model. Your task is to identify the errors in the `PoorPerformingCNN` class and correct them to ensure the model works properly for the CIFAR-10 dataset.\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/poor_cnn.py**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d190e2",
   "metadata": {
    "id": "3qRf7YUAr49u"
   },
   "outputs": [],
   "source": [
    "from libs.models import PoorPerformingCNN\n",
    "\n",
    "net = PoorPerformingCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb7e81",
   "metadata": {
    "id": "CwRxKCRMsUiU"
   },
   "source": [
    "Loss Function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73233d5",
   "metadata": {
    "id": "lQ_9fKCfsV3X"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b442d3",
   "metadata": {
    "id": "RSm7aXCJuE1I"
   },
   "source": [
    "### 4.3: Training procedure\n",
    "\n",
    "In this exercise, you will complete the training and validation loop of a neural network model. Your task is to compute and store the average training loss, average validation loss, and the corresponding accuracies for each epoch.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "* **Training Phase**:\n",
    "After computing the average training loss (avg_train_loss), you need to calculate the training accuracy based on the model's predictions and append the calculated training accuracy to the train_accuracies list.\n",
    "\n",
    "* **Validation Phase**:\n",
    "After calculating the average validation loss (avg_val_loss), you need to calculate the validation accuracy based on the validation dataset and append the calculated validation accuracy to the val_accuracies list. (the same as befor but for the val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3858bb39",
   "metadata": {
    "id": "RSUv7VMdsblF"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (256) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (256) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # Training Phase\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    #####################################################\n",
    "    ##              YOUR CODE HERE                     ##\n",
    "    #####################################################\n",
    "    \n",
    "    # Calculate average loss for the training epoch\n",
    "    avg_train_loss = running_loss / total_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    _, y_pred = torch.max(outputs.data, 1)\n",
    "    train_accuracy = correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    #####################################################\n",
    "    ##              END OF YOUR CODE                   ##\n",
    "    #####################################################\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation Phase\n",
    "    net.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data in val_loader:\n",
    "            val_inputs, val_labels = val_data\n",
    "\n",
    "            # Forward pass\n",
    "            val_outputs = net(val_inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ##              YOUR CODE HERE                     ##\n",
    "    #####################################################\n",
    "\n",
    "  # Calculate average loss for the validation epoch\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    #####################################################\n",
    "    ##              END OF YOUR CODE                   ##\n",
    "    #####################################################\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb35d88",
   "metadata": {
    "id": "WhmPPQf9sh37"
   },
   "source": [
    "### 4.4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e248e772",
   "metadata": {
    "id": "Bo6vZmsUsjyr"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Count the number of correct predictions\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Cuenta las predicciones correctas\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m#####################################################\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m##              END OF YOUR CODE                   ##\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m#####################################################\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy on the test images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        #####################################################\n",
    "        ##              YOUR CODE HERE                     ##\n",
    "        #####################################################\n",
    "\n",
    "        # Calculate total by the batch size (number of samples in the batch)\n",
    "        total += labels.size(0) \n",
    "        # Count the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()  # Cuenta las predicciones correctas\n",
    "\n",
    "\n",
    "        #####################################################\n",
    "        ##              END OF YOUR CODE                   ##\n",
    "        #####################################################\n",
    "\n",
    "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9755d",
   "metadata": {
    "id": "7kX4nLdOssWw"
   },
   "source": [
    "### 4.5: Report\n",
    "\n",
    "1. What challenges can class imbalance introduce when training a machine learning model?\n",
    "\n",
    "2. What are some strategies to address class imbalance in your dataset or training process?\n",
    "\n",
    "3. Why might accuracy alone be misleading as a performance measure in the presence of class imbalance\n",
    "\n",
    "4. Is the cifar-10 and imbalanced dataset? plot the number of samples for each classes inside the cifar 10 dataset.\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "\n",
    "**WRITE YOUR ANSWER HERE:**\n",
    "\n",
    "1. Class imbalance can introduce different challenges when training a ML model, including some of the following:\n",
    "   - **Bias toward the majority class**: The model may become biased toward predicting the majority class because it sees far more examples of this class during training, which can lead to a worse performance on the minority class.\n",
    "   - **Reduced generalization**: The model may fail to generalize well to underrepresented classes, as it may not learn enough distinguishing features for these classes.\n",
    "   - **Evaluation metrics distortion**: Performance metrics like accuracy can be misleading in imbalanced datasets, as the model could achieve high accuracy simply by predicting the majority class most of the time.\n",
    "\n",
    "2. Some common strategies to address class imbalance are:\n",
    "   - **Oversampling**: This involves increasing the number of samples in the minority class. This can be done by either duplicating existing samples or generating synthetic data\n",
    "   - **Undersampling**: This method reduces the number of samples in the majority class to balance the dataset. However, one downside is that it may result in the loss of important data, potentially affecting model performance.\n",
    "   - **Class Weights**: By adjusting the loss function, we can give higher penalties to misclassifications of the minority class. This helps the model pay more attention to the underrepresented classes during training.\n",
    "   - **Ensemble Methods**: Techniques like random forests or boosting algorithms are effective when dealing with imbalanced datasets, as they combine multiple models to improve accuracy.\n",
    "\n",
    "\n",
    "3. Accuracy can be a misleading metric because it doesn't account for the differences in performance between the majority and minority classes. In the case of an imbalanced dataset, if the model predominantly predicts the majority class, it could still report high accuracy, even if it fails to classify the minority class properly. For example, in a dataset where 95% of the samples belong to one class, a model that always predicts the majority class would achieve 95% accuracy, despite not addressing the minority class at all. Therefore, to have a clearer perspective of the model's effectiveness, metrics like precision, recall, and F1-score are more informative, as they better reflect performance, particularly for imbalanced datasets.\n",
    "\n",
    "\n",
    "4. The CIFAR-10 dataset is fairly balanced compared to many other datasets when it comes to the number of samples per class. It contains 10 different classes, with each class having 6,000 images in total (5,000 for training and 1,000 for testing). The distribution of samples is quite even across all classes, which means that there is no significant class imbalance in CIFAR-10. To verify this, we can visualize the number of samples in each class of the CIFAR-10 dataset by plotting them:\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd907b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Get the labels from the dataset\n",
    "labels = [label for _, label in trainset]\n",
    "\n",
    "# Count the number of samples for each class\n",
    "class_counts = Counter(labels)\n",
    "\n",
    "# Plot the number of samples per class\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Number of Samples per Class in CIFAR-10 Dataset')\n",
    "\n",
    "# Set <y> limits to make sure the bars fit\n",
    "plt.ylim(0, 6000)\n",
    "\n",
    "# Set the class names on the x-axis\n",
    "plt.xticks(range(10), ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb829e",
   "metadata": {
    "id": "zgB9zLE6s04N"
   },
   "source": [
    "## **5: Improve the accuracy** (BONUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c289bcd",
   "metadata": {
    "id": "Lm7RZNOltsWf"
   },
   "source": [
    "### 5.1: Custom model\n",
    "\n",
    "After successfully debugging the model, you'll notice that the accuracy on the CIFAR-10 dataset is only around 50-60%. Your second task is to improve the model's performance.\n",
    "\n",
    "How?\n",
    "\n",
    "*   Add more convolutional layers to capture higher-level features.\n",
    "*   Use Batch Normalization\n",
    "*   Add Dropout Layers\n",
    "\n",
    "Data Augmentation:\n",
    "*   Apply transformations like random cropping, flipping, and rotation.\n",
    "\n",
    "Hint: You CAN implement already pre-existing CNN architectures (do your research). As long as it is a CNN everything is fine.\n",
    "\n",
    "By the end of this section you should return the accuracy of your model on the test dataset.\n",
    "\n",
    "NB: by better score we mean at least +10% with respect to the previous model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ea8c6",
   "metadata": {
    "id": "CZNA68C5s2Is"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# transformations defined to flip images, crop them and normalie the values\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486bead",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "\n",
    "**Fill in the code in `libs/models/custom_cnn.py**\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30575ec4",
   "metadata": {
    "id": "5TX-5wRmtIAt"
   },
   "outputs": [],
   "source": [
    "from libs.models import CustomCNN\n",
    "\n",
    "net = CustomCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba4705",
   "metadata": {
    "id": "FstE7TPStY-n"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba8675",
   "metadata": {
    "id": "zDjiBMdTtVcI"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=5e-05)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ceb57",
   "metadata": {
    "id": "vi3EBTq_tXYL"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7550f6",
   "metadata": {
    "id": "mNOjqxdwtW99"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c58dfb",
   "metadata": {
    "id": "KICkuQv9tbJk"
   },
   "source": [
    "\n",
    "### 5.2: Pretrained network\n",
    "In this exercise, you will start from scratch to adapt a pre-trained AlexNet model for the CIFAR-10 dataset.\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Use torchvision.models to load a pre-trained AlexNet. Be sure to specify that the model should be pre-trained on ImageNet.\n",
    "\n",
    "- The CIFAR-10 dataset has 10 classes, so you need to update the model‚Äôs final layer to output 10 classes instead of the default 1000.\n",
    "\n",
    "- Replace the final fully connected layer in AlexNet‚Äôs classifier to output 10 classes.\n",
    "\n",
    "- To perform fine-tuning, freeze all layers except the newly added fully connected layer.\n",
    "\n",
    "- Move your model to the appropriate device (cuda if available). Define a device and ensure the model is moved to that device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ee586",
   "metadata": {
    "id": "3DsgJ10RtjPS"
   },
   "outputs": [],
   "source": [
    "# Define transformations for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8400e",
   "metadata": {
    "id": "eLCv9hY7tkBG"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##              YOUR CODE HERE                     ##\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "# Define the device (determines whether to use GPU or CPU for computation)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained AlexNet model (pretrained on ImageNet for feature extraction)\n",
    "model = models.alexnet(pretrained=True)\n",
    "\n",
    "# Modify the classifier to output 10 classes (CIFAR-10 has 10 categories, replacing the original 1000-class output)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "\n",
    "# Freeze all layers except the newly added fully connected layer (we only want to train the last layer for fine-tuning)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "##              END OF YOUR CODE                   ##\n",
    "#####################################################\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bfc3c4",
   "metadata": {
    "id": "5-t3hEZGtmSo"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-05)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
